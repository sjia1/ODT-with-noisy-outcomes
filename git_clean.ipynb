{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Part 1: Read data  ######################################\n",
    "## with open(\"wiser_clean.csv\") as csvfile:\n",
    "with open(\"wiser_clean.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter = \",\")\n",
    "    input_table = list(reader)\n",
    "\n",
    "## read 3 prior distr into a single dict.\n",
    "with open(\"wiser_prior.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter = \",\")\n",
    "    prior = list(reader)\n",
    "    \n",
    "which_prior = 0 # choose from 0,1,2 since we have 3 priors\n",
    "\n",
    "## convert prior from dict to an array (so that we can perform inner product)\n",
    "m = len(input_table) # scenarios = 255 for input_table\n",
    "n = len(input_table[0]) - 1 # test  This should be 78 for input_table\n",
    "q = np.zeros(m) # prior\n",
    "for j in range(m):\n",
    "    q[j] = float(prior[j][str(which_prior)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('func_su.py').read()) ## load the basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we estimate the ''greedy score'' of a test. Let $E$ be the tests we already selected and $T$ be a test, then $$G(T,E) = \\sum_{x}\\pi_x \\sum_{b:b\\sim x} 2^{-m_x} gain(b,x;T,E) = \\mathbb{E}_x \\big[\\mathbb{E}_{b\\sim x} [gain(b,x;T,E)]\\big],$$\n",
    "where \n",
    "$$gain(b,x;T,E) = \\frac{f_{b,x}(E\\cup T) - f_{b,x}(E)}{1-f_{b,x}(E)} = \\frac{\\text{# extra scenarios ruled out by}\\ T}{\\text{# alive after performing}\\ E},$$ if $f_{b,x}(E)<1$, and $0$ otherwise. \n",
    "\n",
    "This sum involoves exponentially many terms, so we apply Monto Carlo to estimate it. To be precise, for each $x\\in [m]$, we will estimate $\\mathbb{E}_{b\\sim x} [gain(b,x;T,E)]$ by running Monte Carlo for num_sample epochs, and then sum over $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune-able parameters\n",
    "#num_sample_MC = 5; num_sample_ECT = 10; num_rep = 10\n",
    "num_sample_MC = 1; num_sample_ECT = 1; num_rep = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall that if all tests have \"low\" greedy score, then the future ordering doesn't matter much. Here we set this bar at $10^{-1}$. You should expect the greedy score to be decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at round 0 greedy chooses test 52 with greedy score 0.4850750172470581\n"
     ]
    }
   ],
   "source": [
    "############################## Part 3: Compute Greedy Permutation  ######################################\n",
    "skip_stupid_test = 0\n",
    "\n",
    "for i in range(num_rep): # repeat for num_rep times\n",
    "    E = [] # tests chosen so far\n",
    "    not_chosen = list(range(n)) # tests NOT chosen so far\n",
    "    stop_flag = 0 # when max_gain is too small, we stop and randomly permute the rem- tests\n",
    "\n",
    "    for t in range(n):\n",
    "        best_test = random.choice(not_chosen) # randomly choose a test T to start with, if nobody else beats it, then T wins\n",
    "        if stop_flag == 0: \n",
    "            max_gain_so_far = estimate_G(best_test, E, num_sample_MC) # start with the greedy score of this test\n",
    "            for T in range(n):\n",
    "                if T in not_chosen: # only choose from unchosen tests\n",
    "                    temp = estimate_G(T, E, num_sample_MC)\n",
    "                    if temp > max_gain_so_far: # if T beats the best test\n",
    "                        max_gain_so_far = temp\n",
    "                        best_test = T\n",
    "            print(\"at round\", t, \"greedy chooses test\", best_test, \n",
    "                  \"with greedy score\", max_gain_so_far)\n",
    "            \n",
    "            if max_gain_so_far <= 10**(-1): # if all tests have \"low\" greedy score, then the future ordering doesn't matter much\n",
    "                stop_flag = 1\n",
    "        E.append(best_test)\n",
    "        not_chosen.remove(best_test)\n",
    "    print(\"done!\")\n",
    "\n",
    "## find the average cover time of the greedy permutation.\n",
    "    ECT = np.zeros(m) # store the ECT of each scenario\n",
    "    progress_bar_freq = 50 # how often do you want to desplay progress\n",
    "    for x_hat in range(m):\n",
    "        temp = 0 # store the sum of cover times (over all epochs)\n",
    "        for k in range(num_sample_ECT): # Monto Carlo the ECT of x_hat for num_sample_ECT times and take average\n",
    "            outcome = gen_b(x_hat) # generate outcome vector\n",
    "            temp += compute_cover_time(E, outcome, skip_stupid_test) # E = greedy perm computed above\n",
    "\n",
    "        ECT[x_hat] = temp/num_sample_ECT\n",
    "\n",
    "    print(\"avg cover time of greedy perm:\", np.matmul(q, ECT)) # q = prior distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
